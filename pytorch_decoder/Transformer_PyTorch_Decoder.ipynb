{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2ab4181c6314d329019cbec48e4ee23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43f6a372a8234b39b00d30c790cd021b",
              "IPY_MODEL_9d7616492b1c475baaf1475392d177e0",
              "IPY_MODEL_bdab4876abaa49f7880e681b2acf5acb"
            ],
            "layout": "IPY_MODEL_d792bcdd08dc4b5dac2f30fbf9086421"
          }
        },
        "43f6a372a8234b39b00d30c790cd021b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cfadc8d477740ba9f8efe911a398dd3",
            "placeholder": "​",
            "style": "IPY_MODEL_03932c5330e64833ac6dbfb9ea1dbfc8",
            "value": "Map: 100%"
          }
        },
        "9d7616492b1c475baaf1475392d177e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d7aa0bf0d194e4894170ae79468e9db",
            "max": 872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5cc0695e704248d1b16a7dd260abe42d",
            "value": 872
          }
        },
        "bdab4876abaa49f7880e681b2acf5acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_046564dc0ba84539bad654358b7f98fb",
            "placeholder": "​",
            "style": "IPY_MODEL_140808b69c5a45bdb912a033cc9fed93",
            "value": " 872/872 [00:00&lt;00:00, 4593.77 examples/s]"
          }
        },
        "d792bcdd08dc4b5dac2f30fbf9086421": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cfadc8d477740ba9f8efe911a398dd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03932c5330e64833ac6dbfb9ea1dbfc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d7aa0bf0d194e4894170ae79468e9db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cc0695e704248d1b16a7dd260abe42d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "046564dc0ba84539bad654358b7f98fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "140808b69c5a45bdb912a033cc9fed93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmTUeCZzmJzr"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self,d_k,d_model,n_heads,max_len):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_k=d_k\n",
        "    self.n_heads=n_heads\n",
        "\n",
        "    self.key= nn.Linear(d_model, d_k * n_heads)\n",
        "    self.query= nn.Linear(d_model, d_k * n_heads)\n",
        "    self.value= nn.Linear(d_model, d_k * n_heads)\n",
        "\n",
        "    self.fc=nn.Linear(d_k * n_heads, d_model)\n",
        "\n",
        "    cm=torch.tril(torch.ones(max_len, max_len))\n",
        "    self.register_buffer(\n",
        "        \"causal_mask\",\n",
        "        cm.view(1, 1, max_len, max_len)\n",
        "    )\n",
        "  def forward(self,q,k,v, pad_mask=None):\n",
        "    q=self.query(q)\n",
        "    k=self.key(k)\n",
        "    v=self.value(v)\n",
        "\n",
        "    N=q.shape[0]\n",
        "    T=q.shape[1]\n",
        "\n",
        "    q=q.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
        "    k=k.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
        "    v=v.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    attn_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
        "    if pad_mask is not None:\n",
        "      attn_scores= attn_scores.masked_fill(\n",
        "          pad_mask[:, None, None,: ] == 0 , float('-inf'))\n",
        "    attn_scores = attn_scores.masked_fill(\n",
        "        self.causal_mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    A = attn_weights @ v\n",
        "    A = A.transpose(1,2)\n",
        "    A = A.contiguous().view(N,T, self.d_k * self.n_heads)\n",
        "\n",
        "    return self.fc(A)"
      ],
      "metadata": {
        "id": "-wTYdSlsmZCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ln1= nn.LayerNorm(d_model)\n",
        "    self.ln2= nn.LayerNorm(d_model)\n",
        "    self.mha= CausalSelfAttention(d_k, d_model, n_heads, max_len)\n",
        "    self.ann= nn.Sequential(\n",
        "        nn.Linear(d_model, d_model* 4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(d_model * 4, d_model),\n",
        "        nn.Dropout(dropout_prob),\n",
        "    )\n",
        "    self.dropout=nn.Dropout(p=dropout_prob)\n",
        "\n",
        "  def forward(self, x, pad_mask=None):\n",
        "    x = self.ln1(x + self.mha(x,x,x, pad_mask))\n",
        "    x= self.ln2(x + self.ann(x))\n",
        "    x=self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "CjcXHXUW7GfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model,max_len=2048, dropout_prob=0.1):\n",
        "    super().__init__()\n",
        "    self.dropout= nn.Dropout(dropout_prob)\n",
        "\n",
        "    position= torch.arange(max_len).unsqueeze(1)\n",
        "    exp_term =torch.arange(0, d_model, 2)\n",
        "    div_term= torch.exp(exp_term * (-math.log(10000.0)/ d_model))\n",
        "    pe =torch.zeros(1,max_len,d_model)\n",
        "    pe[0,:,0::2]=torch.sin(position * div_term)\n",
        "    pe[0, :, 1::2]=torch.sin(position * div_term)\n",
        "    self.register_buffer('pe',pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x= x+self.pe[:, :x.size(1), :]\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "lZquPCni9atl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               max_len,\n",
        "               d_k,\n",
        "               d_model,\n",
        "               n_heads,\n",
        "               n_layers,\n",
        "               dropout_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding= nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
        "    transformer_blocks=[\n",
        "        TransformerBlock(\n",
        "            d_k,\n",
        "            d_model,\n",
        "            n_heads,\n",
        "            max_len,\n",
        "            dropout_prob\n",
        "        ) for _ in range(n_layers)\n",
        "    ]\n",
        "    self.transformer_blocks= nn.Sequential(*transformer_blocks)\n",
        "    self.ln= nn.LayerNorm(d_model)\n",
        "    self.fc= nn.Linear(d_model,vocab_size)\n",
        "\n",
        "  def forward(self, x, pad_mask=None):\n",
        "    x=self.embedding(x)\n",
        "    x=self.pos_encoding(x)\n",
        "    for block in self.transformer_blocks:\n",
        "      x=block(x, pad_mask)\n",
        "    x=self.ln(x)\n",
        "    x=self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "YI0xCcbeBbvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= Decoder(20000, 1024, 16, 64, 4, 2, 0.1)"
      ],
      "metadata": {
        "id": "8Q3r9t9_DV3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIhOD81fDiNc",
        "outputId": "58aca73f-8f31-4691-c3a7-e6889316216b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(20000, 64)\n",
              "  (pos_encoding): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): CausalSelfAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): CausalSelfAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=64, out_features=20000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randint(0, 20_000, size=(8, 512))\n",
        "x_t = torch.tensor(x).to(device)"
      ],
      "metadata": {
        "id": "n6Y1l_yaD4P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=model(x_t)\n",
        "y.shape"
      ],
      "metadata": {
        "id": "vbNU5JcFEJeC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b8470f-0588-4fa2-9f12-47240717261b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 512, 20000])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.ones((8, 512))\n",
        "mask[:, 256:] = 0\n",
        "mask_t = torch.tensor(mask).to(device)"
      ],
      "metadata": {
        "id": "A704jFHcEOE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEDypxgHnB8X",
        "outputId": "5f0e9991-621c-4468-8781-564087e6bb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer , DataCollatorWithPadding"
      ],
      "metadata": {
        "id": "gH2atFa60pRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "checkpoint = 'distilbert-base-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRX8elMlnK5F",
        "outputId": "fe8599e4-39df-4c30-e055-057275f543fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "A110GVMIneQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll use the same dataset, just ignore the labels\n",
        "raw_datasets = load_dataset(\"glue\", \"sst2\")"
      ],
      "metadata": {
        "id": "NZS-A0tGniVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj07LyBknqvm",
        "outputId": "8adea1da-1cd2-4fa0-a2db-33ad7a75c7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 67349\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 872\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 1821\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(batch):\n",
        "  return tokenizer(batch['sentence'], truncation=True)"
      ],
      "metadata": {
        "id": "lEjXxb-Lntvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c2ab4181c6314d329019cbec48e4ee23",
            "43f6a372a8234b39b00d30c790cd021b",
            "9d7616492b1c475baaf1475392d177e0",
            "bdab4876abaa49f7880e681b2acf5acb",
            "d792bcdd08dc4b5dac2f30fbf9086421",
            "7cfadc8d477740ba9f8efe911a398dd3",
            "03932c5330e64833ac6dbfb9ea1dbfc8",
            "3d7aa0bf0d194e4894170ae79468e9db",
            "5cc0695e704248d1b16a7dd260abe42d",
            "046564dc0ba84539bad654358b7f98fb",
            "140808b69c5a45bdb912a033cc9fed93"
          ]
        },
        "id": "yHCGKARMn_Ot",
        "outputId": "9092a759-caab-450c-8e96-fb90d7d441c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2ab4181c6314d329019cbec48e4ee23"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWyr4F2noP6v",
        "outputId": "5640c763-2cd0-42c6-b897-7067f680f3b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 67349\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 872\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1821\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns(\n",
        "    [\"sentence\", \"idx\", \"label\"])"
      ],
      "metadata": {
        "id": "QnECrxhloSUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=32,\n",
        "    collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "J1u8v5k9ofBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check how it works\n",
        "for batch in train_loader:\n",
        "  for k, v in batch.items():\n",
        "    print(\"k:\", k, \"v.shape:\", v.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3tKqvEIoz3v",
        "outputId": "e48208ea-3363-47fa-ba48-42b471f1e059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k: input_ids v.shape: torch.Size([32, 51])\n",
            "k: attention_mask v.shape: torch.Size([32, 51])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ2y-J-FpExQ",
        "outputId": "f7175f4c-f730-470d-ebf3-04a8785402f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Decoder(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_len=tokenizer.max_model_input_sizes[checkpoint],\n",
        "    d_k=16,\n",
        "    d_model=64,\n",
        "    n_heads=4,\n",
        "    n_layers=2,\n",
        "    dropout_prob=0.1,\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRtkerDepH0G",
        "outputId": "00359243-33a6-43a6-e5f5-3bbd48967330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(28996, 64)\n",
              "  (pos_encoding): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): CausalSelfAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): CausalSelfAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=64, out_features=28996, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "KeILGq_dpkyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "EV8mMoxYsLHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion,optimizer,train_loader,epochs):\n",
        "  train_losses = np.zeros(epochs)\n",
        "\n",
        "  for it in range(epochs):\n",
        "    model.train()\n",
        "    t0= datetime.now()\n",
        "    train_loss=[]\n",
        "    for batch in train_loader:\n",
        "      batch={k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      targets = batch['input_ids'].clone().detach()\n",
        "      targets = torch.roll(targets, shifts=-1,dims=1)\n",
        "      targets[:,-1]= tokenizer.pad_token_id\n",
        "\n",
        "      outputs = model(batch['input_ids'], batch['attention_mask'])\n",
        "      loss= criterion(outputs.transpose(2,1),targets)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss.append(loss.item())\n",
        "\n",
        "    train_loss=np.mean(train_loss)\n",
        "\n",
        "    train_losses[it]= train_loss\n",
        "    dt=datetime.now() - t0\n",
        "    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, Duration: {dt}')\n",
        "\n",
        "  return train_losses"
      ],
      "metadata": {
        "id": "zoDfW2VysRiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = train(\n",
        "    model, criterion, optimizer, train_loader, epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg7NY39ywwHI",
        "outputId": "b62d705d-efb3-4d23-eb8e-38cffcf40a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Train Loss: 4.4477, Duration: 0:00:22.234026\n",
            "Epoch 2/15, Train Loss: 4.4461, Duration: 0:00:22.073419\n",
            "Epoch 3/15, Train Loss: 4.3855, Duration: 0:00:22.385481\n",
            "Epoch 4/15, Train Loss: 4.5376, Duration: 0:00:22.670897\n",
            "Epoch 5/15, Train Loss: 4.7858, Duration: 0:00:24.048303\n",
            "Epoch 6/15, Train Loss: 4.2954, Duration: 0:00:22.235688\n",
            "Epoch 7/15, Train Loss: 4.4654, Duration: 0:00:26.513532\n",
            "Epoch 8/15, Train Loss: 4.3499, Duration: 0:00:25.815533\n",
            "Epoch 9/15, Train Loss: 4.4842, Duration: 0:00:27.207444\n",
            "Epoch 10/15, Train Loss: 4.1911, Duration: 0:00:25.701373\n",
            "Epoch 11/15, Train Loss: 4.5511, Duration: 0:00:22.059607\n",
            "Epoch 12/15, Train Loss: 4.3957, Duration: 0:00:22.121972\n",
            "Epoch 13/15, Train Loss: 4.2381, Duration: 0:00:22.721953\n",
            "Epoch 14/15, Train Loss: 4.3993, Duration: 0:00:22.260906\n",
            "Epoch 15/15, Train Loss: 4.6204, Duration: 0:00:22.349653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_loader = DataLoader(\n",
        "    tokenized_datasets['validation'],\n",
        "    batch_size=1,\n",
        "    collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "_8W79i8ow5Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "for batch in valid_loader:\n",
        "  batch= {k: v.to(device) for k,v in batch.items()}\n",
        "  outputs = model(batch['input_ids'],batch['attention_mask'])\n",
        "  break"
      ],
      "metadata": {
        "id": "sLxi-vP-xKRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkK9EptvChEe",
        "outputId": "bdc1b1f0-4407-4268-c28a-6ecad9fe7636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 12, 28996])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(outputs, axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0sGkzB4CrBg",
        "outputId": "2148ae9d-4644-4a7f-cd8f-1774b73177f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  170,   112,   188,   170,  2523,   117,   102, 20714,   102,   102,\n",
              "           102,  1104]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_ids = torch.argmax(outputs, axis=-1)"
      ],
      "metadata": {
        "id": "RsigR-r6CsU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(prediction_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2zj-j2iYCs7o",
        "outputId": "6a374335-1672-477e-b540-33d3687af03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"a's a movie, [SEP] awkwardly [SEP] [SEP] [SEP] of\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(batch['input_ids'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "y6K9Jr_CCvPK",
        "outputId": "485a1d34-6a12-4013-e0fc-793cd9d8a1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[CLS] it's a charming and often affecting journey. [SEP]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(torch.concat((batch['input_ids'][0, :5], prediction_ids[:, 4])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qOgGfP-BCzxf",
        "outputId": "81d6027e-a5df-4e81-8a2b-dc79e79922a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[CLS] it's a movie\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate something\n",
        "prompt = \"it's\"\n",
        "\n",
        "tokenized_prompt = tokenizer(prompt, return_tensors='pt')\n",
        "tokenized_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p08hhzssC2jN",
        "outputId": "ca32ece3-7f51-4117-c041-b320f20ca50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 1122,  112,  188,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(\n",
        "    tokenized_prompt['input_ids'][:, :-1].to(device),\n",
        "    tokenized_prompt['attention_mask'][:, :-1].to(device))\n",
        "\n",
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzfrAplIC5EC",
        "outputId": "0d11555d-c326-4f8c-f9f5-7c3ea69e85b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 28996])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_ids = torch.argmax(outputs[:, -1, :], axis=-1)"
      ],
      "metadata": {
        "id": "010MFi0FC6aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(prediction_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pCR5wIJFC6-W",
        "outputId": "b5abb88d-11b8-4f1c-b33e-618730eb5460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate something\n",
        "prompt = \"it's a\"\n",
        "\n",
        "tokenized_prompt = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "# prepare inputs + get rid of SEP token at the end\n",
        "input_ids = tokenized_prompt['input_ids'][:, :-1].to(device)\n",
        "mask = tokenized_prompt['attention_mask'][:, :-1].to(device)\n",
        "\n",
        "for _ in range(20):\n",
        "  outputs = model(input_ids, mask)\n",
        "  prediction_id = torch.argmax(outputs[:, -1, :], axis=-1)\n",
        "\n",
        "  input_ids = torch.hstack((input_ids, prediction_id.view(1, 1)))\n",
        "  mask = torch.ones_like(input_ids)\n",
        "\n",
        "  if prediction_id == tokenizer.sep_token_id:\n",
        "    break"
      ],
      "metadata": {
        "id": "vrdE2hwNC-2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(input_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6tuxBmYaDANW",
        "outputId": "af46322b-6f5e-4f21-8e6a-db120638a449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[CLS] it's a movie that is a movie that is a movie that is a movie that is a movie that is a\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}